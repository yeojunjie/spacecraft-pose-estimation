{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "CHAIN_ID = \"0a998b28bd\"\n",
    "PATH_TO_FOLDER = \"/Users/yeojunjie/Documents/NUS Modules/CS3263/Project/spacecraft-pose-estimation/data/images/\" + CHAIN_ID + \"/\"\n",
    "\n",
    "# The camera intrinsic matrix.\n",
    "# Source: https://www.drivendata.org/competitions/261/spacecraft-pose-estimation/page/834/#camera-intrinsic-parameters\n",
    "K = np.array([[5.2125371e+03, 0.0000000e+00, 6.4000000e+02],\n",
    "              [0.0000000e+00, 6.2550444e+03, 5.1200000e+02],\n",
    "              [0.0000000e+00, 0.0000000e+00, 1.0000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a rotation matrix to a quaternion.\n",
    "# Source: https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions#Rotation_matrix_â†”_quaternion\n",
    "def rotation_matrix_to_quaternion(R):\n",
    "    q = np.empty((4,))\n",
    "    q[0] = np.sqrt(1 + R[0, 0] + R[1, 1] + R[2, 2]) / 2\n",
    "    q[1] = (R[2, 1] - R[1, 2]) / (4 * q[0])\n",
    "    q[2] = (R[0, 2] - R[2, 0]) / (4 * q[0])\n",
    "    q[3] = (R[1, 0] - R[0, 1]) / (4 * q[0])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWES_RATIO = 0.5\n",
    "\n",
    "# Get the translation vector and rotation quartenion between the i-th and the j-th image in the list.\n",
    "# (In our usage, i + 1 = j.)\n",
    "def get_translation_and_rotation(list, i, j):\n",
    "    assert i < len(list) and j < len(list)\n",
    "\n",
    "    # Convert the images to grayscale. (Why am I even doing this?)\n",
    "    gray_i = cv.cvtColor(list[i], cv.COLOR_BGR2GRAY)\n",
    "    gray_j = cv.cvtColor(list[j], cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the keypoints and descriptors with SIFT.\n",
    "    sift = cv.SIFT_create()\n",
    "    keypoints_i, descriptors_i = sift.detectAndCompute(gray_i, None)\n",
    "    keypoints_j, descriptors_j = sift.detectAndCompute(gray_j, None)\n",
    "\n",
    "    # Create a BFMatcher object and match the descriptors.\n",
    "    bf = cv.BFMatcher()\n",
    "    matches = bf.knnMatch(descriptors_i, descriptors_j, k=2)\n",
    "    # k = 2 means that we get the two best matches.\n",
    "    # We compare how similar the best match is to the second best match to determine if the best match is unambiguous ('good').\n",
    "\n",
    "    # Sort the matches by decreasing ambiguity.\n",
    "    matches.sort(key = lambda x: x[0].distance / x[1].distance, reverse = False)\n",
    "\n",
    "    # Retain the 15 best matches.\n",
    "    matches = matches[:15]\n",
    "\n",
    "    print(\"At iteration i = \" + str(i) + \", the number of good matches is \" + str(len(matches)) + \".\")\n",
    "    \n",
    "    # Extract corresponding points.\n",
    "    points_i = np.float32([keypoints_i[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    points_j = np.float32([keypoints_j[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Find the fundamental matrix F.\n",
    "    F, mask = cv.findFundamentalMat(points_i, points_j, cv.FM_RANSAC, 0.1, 0.99)\n",
    "\n",
    "    # Really dodgy error handling: If F is None, it means that the fundamental matrix could not be calculated.\n",
    "    if F is None:\n",
    "        print(\"Could not calculate the fundamental matrix at iteration i = \" + str(i) + \".\")\n",
    "        # Hence, we just return the zero vector and the identity matrix.\n",
    "        return np.zeros((3, 1)), np.eye(3)\n",
    "        \n",
    "    # Calculate the essential matrix E.\n",
    "    E = np.dot(np.dot(K.T, F), K)\n",
    "\n",
    "    # Decompose E to get the rotation matrix R and the translation matrix T.\n",
    "    _, R, T, mask = cv.recoverPose(E, points_i, points_j, K)\n",
    "\n",
    "    return T, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose rotation matrices and translation vectors.\n",
    "def compose_rotation_and_translation(T1, R1, T2, R2):    \n",
    "    \n",
    "    # Compose the rotation matrices.\n",
    "    R = R2 @ R1\n",
    "\n",
    "    # Compose the translation vectors.\n",
    "    T = R2 @ T1.T + T2.T\n",
    "\n",
    "    return T, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100 images.\n",
    "images = []\n",
    "for i in range(0, 100):\n",
    "    PATH_TO_IMAGE = os.path.join(PATH_TO_FOLDER, \"{:03d}.png\".format(i))\n",
    "    image = cv.imread(PATH_TO_IMAGE)\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Iterate through the images.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Get the translation vector and the rotation matrix.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     T_i, R_i \u001b[38;5;241m=\u001b[39m \u001b[43mget_translation_and_rotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Compose the translation vector and the rotation matrix.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     T, R \u001b[38;5;241m=\u001b[39m compose_rotation_and_translation(T, R, T_i, R_i)\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mget_translation_and_rotation\u001b[0;34m(list, i, j)\u001b[0m\n\u001b[1;32m     19\u001b[0m matches \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mknnMatch(descriptors_i, descriptors_j, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# k = 2 means that we get the two best matches.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# We compare how similar the best match is to the second best match to determine if the best match is unambiguous ('good').\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Sort the matches by decreasing ambiguity.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m(key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m/\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdistance, reverse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Retain the 15 best matches.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m matches \u001b[38;5;241m=\u001b[39m matches[:\u001b[38;5;241m15\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "# Calculate and report the results.\n",
    "\n",
    "# Create a .csv file with header \"chain_id,i,x,y,z,qw,qx,qy,qz\".\n",
    "with open(\"results.csv\", \"w\") as file:\n",
    "    file.write(\"chain_id,i,x,y,z,qw,qx,qy,qz\\n\")\n",
    "\n",
    "    # Initialize the translation vector and the rotation matrix.\n",
    "    # These two variables will be updated as we iterate through the chain of 100 images.\n",
    "    T = np.zeros((3,))\n",
    "    R = np.eye(3)\n",
    "\n",
    "    # Write the line for i = 0.\n",
    "    file.write(CHAIN_ID + \",0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\\n\")\n",
    "\n",
    "    # Iterate through the images.\n",
    "    for i in range(1, 100):\n",
    "        # Get the translation vector and the rotation matrix.\n",
    "        T_i, R_i = get_translation_and_rotation(images, i - 1, i)\n",
    "\n",
    "        # Compose the translation vector and the rotation matrix.\n",
    "        T, R = compose_rotation_and_translation(T, R, T_i, R_i)\n",
    "\n",
    "        # Convert the rotation matrix to a quaternion.\n",
    "        R_q = rotation_matrix_to_quaternion(R)\n",
    "\n",
    "        # Write the results to the .csv file.\n",
    "        file.write(\"{},{},{},{},{},{},{},{},{}\\n\".format(CHAIN_ID, i, T[0][0], T[0][1], T[0][2], R_q[0], R_q[1], R_q[2], R_q[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
